import random
import re

import numpy as np
import pandas as pd
from keras import Input, Model
from keras.layers import Dense, Embedding, Flatten, Reshape, concatenate
from nltk import ngrams
from sklearn.model_selection import train_test_split

from src.utils import create_vocabulary, save_embeddings


def process_data(df, window_size):
    """
    This function uses the ngrams function of sklearn to generate word n-grams, where n is set to the parameter value 
    window_size. For every line in our data file, a unique, ascending document identifier is generated. A context 
    array is built up, containing the document identifier and the numerical representations (integers) of the words 
    in every n-gram. The function also generates a legend file consisting of combinations of document identifiers (
    integers) and raw document content (in our case, our documents are single-line text files).
    
    :param df: 
    :type df: 
    :param window_size: 
    :type window_size: 
    :return: 
    :rtype: 
    """
    vocab = {}
    vocab = create_vocabulary(vocab, df["text"].tolist())
    docid = 0
    contexts = []
    docids = []
    targets = []

    with open("../data/test/docs.legend", "w") as f:
        for _, row in df.iterrows():
            s = row["text"]
            f.write("%d %s\n" % (docid, s))
            docids.append(docid)
            ngs = list(ngrams(s.split(), window_size))
            for i in range(len(ngs) - 1):
                cs = [docid]
                ng = ngs[i]
                for w in ng:
                    w = re.sub("[.,:;'\"!?()]+", "", w.lower())
                    cs.append(vocab[w])
                contexts.append(cs)
                target_word = re.sub("[.,:;'\"!?()]+", "", ngs[i + 1][0].lower())
                targets.append(vocab[target_word])
            docid += 1

    return np.array(contexts), np.array(docids), np.array(targets), vocab


def generator(target, context, batch_size):
    """
    generator function that deploys these arrays to generate batches of training data based on random selections
  
    This function is called during training and generates random batches consisting of 2D arrays (with size (batch 
    size,1)) harboring the words in the contexts generated by collect_data, the corresponding document identifiers, 
    and the target words.
  
    :param target: 
    :type target: 
    :param context: 
    :type context: 
    :param batch_size: 
    :type batch_size: 
    :return: 
    :rtype: 
    """
    w1 = np.zeros((batch_size, 1))
    w2 = np.zeros((batch_size, 1))
    w3 = np.zeros((batch_size, 1))
    docid = np.zeros((batch_size, 1))
    batch_targets = np.zeros((batch_size, 1))

    while True:
        for i in range(batch_size):
            index = random.randint(0, len(target) - 1)
            batch_targets[i] = target[index]
            docid[i] = context[index][0]
            w1[i] = context[index][1]
            w2[i] = context[index][2]
            w3[i] = context[index][3]
        yield [w1, w2, w3, docid], np.array(batch_targets)


EMBEDDING_PATH = "../data/test/d2v_embedding.txt"


def run():
    # dataset = load_dataset("yelp_polarity") from HuggingFace
    df = pd.read_parquet("../data/yelp_polarity.parquet.gzip")
    df = df.sample(frac=0.05)

    def clean_text(sentence):
        sentence = sentence.lower()
        sentence = re.sub(r"[-()\"#/@;:<>{}=~.|?,*]", " ", sentence)
        sentence = sentence.replace("\\n", "")
        sentence = sentence.replace("\\", "")
        # normalize spacing
        sentence = " ".join(sentence.split())
        # sentence = sentence.replace('\"', '')
        # for w in sentence:
        #     w = w.
        return sentence

    df["text"] = df["text"].map(clean_text)
    train, test = train_test_split(df, test_size=0.2)
    print(train.info())
    print(test.info())
    window_size = 3
    vector_dim = 100

    contexts, docids, targets, vocab = process_data(train, window_size)

    vocab_size = len(vocab)

    print("Vocab size: ", vocab_size)

    input_w1 = Input((1,))
    input_w2 = Input((1,))
    input_w3 = Input((1,))
    input_docid = Input((1,))

    # Embeddings
    embedding = Embedding(
        vocab_size,
        vector_dim,
        input_length=1,
        name="embedding",
    )

    vector_dim_doc = vector_dim
    embedding_doc = Embedding(
        len(docids) + 1,
        vector_dim_doc,
        name="embedding_doc",
    )

    docid = embedding_doc(input_docid)
    docid = Reshape((vector_dim_doc, 1))(docid)

    w1 = embedding(input_w1)
    w1 = Reshape((vector_dim, 1))(w1)

    w2 = embedding(input_w2)
    w2 = Reshape((vector_dim, 1))(w2)

    w3 = embedding(input_w3)
    w3 = Reshape((vector_dim, 1))(w3)

    context_docid = concatenate([w1, w2, w3, docid])
    context_docid = Flatten()(context_docid)
    output = Dense(vocab_size, activation="softmax")(context_docid)
    model = Model(
        inputs=[input_w1, input_w2, input_w3, input_docid],
        outputs=output,
    )
    model.compile(
        loss="sparse_categorical_crossentropy",
        optimizer="adam",
        metrics=["acc"],
    )

    print(model.summary())

    model.fit(
        generator(targets, contexts, 100),
        steps_per_epoch=100,
        epochs=20,
    )

    save_embeddings(
        EMBEDDING_PATH,
        embedding_doc.get_weights()[0],
        vocab,
    )


if __name__ == "__main__":
    run()
