{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Suppose that you would like to implement a tool that supports message writing, suggesting the next word while you are typing. Moreover, suppose that you would like the tool to learn from you or from a specific set of documents. Such a tool could be useful not only for providing message-writing assistance, but also for supporting spell checking, extracting common phrases, summarizing, and so on."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from py2neo import Graph\n",
    "\n",
    "# Connect to database\n",
    "url = \"bolt://localhost:7687\"\n",
    "username = os.getenv(\"NEO4J_USER\")\n",
    "password = os.getenv(\"NEO4J_PASSWORD\")\n",
    "database = \"spacy\"\n",
    "\n",
    "graph = Graph(url, auth=(username, password), name=database)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T22:02:03.464807200Z",
     "start_time": "2023-05-17T22:02:03.167142400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "add_constraint = \"CREATE CONSTRAINT FOR (w:Word) REQUIRE w.value IS UNIQUE;\"\n",
    "graph.run(add_constraint)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# doesn't work here use desktop\n",
    "load_data_1 = \"\"\"\n",
    "LOAD CSV FROM \"file:///masc_sentences.tsv\" AS line FIELDTERMINATOR '\\t'\n",
    "CALL {\n",
    "    WITH line\n",
    "    WITH line[6] as sentence\n",
    "    WITH split(sentence, \" \") as words\n",
    "    FOREACH ( idx IN range(0,size(words)-2) |\n",
    "    MERGE (w1:Word {value:apoc.text.clean(words[idx])})\n",
    "    MERGE (w2:Word {value:apoc.text.clean(words[idx+1])})\n",
    "    MERGE (w1)-[r:NEXT]->(w2)\n",
    "      ON CREATE SET r.weight = 1\n",
    "      ON MATCH SET r.weight = r.weight + 1)\n",
    "} IN TRANSACTIONS OF 500 ROWS\n",
    "\"\"\"\n",
    "# without cleaning\n",
    "# Added 170037 labels, created 170037 nodes, set 2553105 properties, created 929137 relationships, completed after 261495 ms.\n",
    "# with cleaning\n",
    "# Added 97209 labels, created 97209 nodes, set 2480277 properties, created 806397 relationships, completed after 229257 ms.\n",
    "\n",
    "\n",
    "# New importing query that uses the sentence identifier\n",
    "# The word nodes are unique, so if you have millions of sentences, this schema will create supernodesâ€”that is,\n",
    "# nodes with millions of relationships coming in, going out, or both.\n",
    "load_data = \"\"\"\n",
    "LOAD CSV FROM \"file:///masc_sentences.tsv\" AS line FIELDTERMINATOR '\\t'\n",
    "CALL {\n",
    "    WITH line\n",
    "    WITH line[6] as sentence, line[2] as sentenceId\n",
    "    WITH split(sentence,\" \") as words, sentenceId\n",
    "    FOREACH ( idx IN range(0,size(words)-2) |\n",
    "    MERGE (w1:Word {value:apoc.text.clean(words[idx])})\n",
    "    MERGE (w2:Word {value:apoc.text.clean(words[idx+1])})\n",
    "    CREATE (w1)-[r:NEXT {sentence: sentenceId}]->(w2))\n",
    "} IN TRANSACTIONS OF 500 ROWS\n",
    "\"\"\"\n",
    "# Added 97209 labels, created 97209 nodes, set 2480277 properties, created 2383068 relationships, completed after 93746 ms.\n",
    "delete = \"\"\"\n",
    "CALL apoc.periodic.iterate(\n",
    "\"MATCH (p:Word) RETURN p\",\n",
    "\"DETACH DELETE p\", {batchSize:500})\n",
    "\"\"\"\n",
    "# Had to add masc_sentences.tsv to the DB server\n",
    "graph.run(load_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "MATCH (w:Word {value: \"how\"})-[e:NEXT]->(w2:Word)\n",
    "RETURN w2.value as next, e.weight as frequency\n",
    "ORDER BY frequency desc\n",
    "LIMIT 3\n",
    "\"\"\"\n",
    "\n",
    "res = graph.run(query).to_data_frame()\n",
    "res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "MATCH (w:Word)-[e:NEXT]->(w2:Word)\n",
    "RETURN apoc.text.clean(w2.value) as next, e.weight as frequency\n",
    "ORDER BY frequency desc\n",
    "LIMIT 100\n",
    "\"\"\"\n",
    "res = graph.run(query).to_data_frame()\n",
    "res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### sentence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "MATCH (w2:Word {value: \"know\"})-[r:NEXT]->(w3:Word {value: \"how\"})-[e:NEXT]-> (w4:Word)\n",
    "WHERE r.sentence = e.sentence\n",
    "RETURN w4.value as next, count(DISTINCT r) as frequency\n",
    "ORDER BY frequency desc\n",
    "LIMIT 3\n",
    "\"\"\"\n",
    "query = \"\"\"\n",
    "MATCH (w1:Word {value: \"you\"})-[a:NEXT]->(w2:Word {value: \"know\"})-[r:NEXT]->(w3:Word {value: \"how\"})-[e:NEXT]->(w4:Word)\n",
    "WHERE a.sentence = r.sentence AND r.sentence = e.sentence\n",
    "RETURN w4.value as next, count(DISTINCT r) as frequency\n",
    "ORDER BY frequency desc\n",
    "LIMIT 3\n",
    "\"\"\"\n",
    "res = graph.run(query).to_data_frame()\n",
    "res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Simple Spacy Example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    i = 1\n",
    "    for sentence in doc.sents:\n",
    "        print(\"-------- Sentence \", i, \"-----------\")\n",
    "        i += 1\n",
    "        for token in sentence:\n",
    "            print(token.idx, \"-\", token.text, \"-\", token.lemma_, \"-\", token.tag_)\n",
    "\n",
    "\n",
    "sentence = \"Marie Curie received the Nobel Prize in Physic in 1903. She became the first woman to win the prize.\"\n",
    "tokenize(sentence)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "\n",
    "# stanza.download(\"en\")\n",
    "\n",
    "def tokenize(text):\n",
    "    nlp = stanza.Pipeline()\n",
    "    # models_dir='stanfordnlp_resources')  # This sets up a default neural pipeline in English\n",
    "    doc = nlp(text)\n",
    "    i = 1\n",
    "    for sentence in doc.sentences:\n",
    "        print(\"--------Sentence \", i, \"-----------\")\n",
    "        i += 1\n",
    "        for token in sentence.tokens:\n",
    "            # print(token.pretty_print())\n",
    "            print(token.id, \"-\", token.text, \"-\", token.words[0].lemma)\n",
    "\n",
    "\n",
    "tokenize(\"Barack Obama was born in Hawaii.  He was elected president in 2008.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "import torch\n",
    "\n",
    "import gc\n",
    "\n",
    "\n",
    "def report_gpu():\n",
    "    print(torch.cuda.list_gpu_processes())\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "report_gpu()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataframe Ingestion Version"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = os.getenv(\"NEO4J_USER\")\n",
    "password = os.getenv(\"NEO4J_PASSWORD\")\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "\n",
    "def get_session(database):\n",
    "    return driver.session(database=database)\n",
    "\n",
    "\n",
    "text = \"Marie Curie received the Nobel Prize in Physic in 1903. She became the first woman to win the prize.\"\n",
    "df = pd.DataFrame([text], columns=[\"raw_text\"])\n",
    "\n",
    "\n",
    "def execute_query(query, params):\n",
    "    results = []\n",
    "    with get_session(\"spacy\") as session:\n",
    "        for items in session.run(query, params):\n",
    "            item = items[\"result\"]\n",
    "            results.append(item)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def create_annotated_text(nlp_json, id=1):\n",
    "    query = \"\"\"MERGE (ann:AnnotatedText {id: $id, nlp_json: $nlp_json})\n",
    "        RETURN id(ann) as result\n",
    "    \"\"\"\n",
    "    params = {\"id\": id, \"nlp_json\": nlp_json}\n",
    "    results = execute_query(query, params)\n",
    "    return results[0]\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def create_graph_object(row):\n",
    "    raw_text = row[\"raw_text\"]\n",
    "    for doc in nlp.pipe([raw_text], disable=[\"ner\"]):\n",
    "        row[\"nlp_json\"] = str(doc.to_json())\n",
    "        annotated_text = create_annotated_text(row[\"nlp_json\"])\n",
    "        i = 1\n",
    "        for sentence in doc.sents:\n",
    "            print(\"-------- Sentence \", i, \"-----------\")\n",
    "            print(annotated_text)\n",
    "            print(sentence.text)\n",
    "            # self.store_sentence(sentence, annotated_text, text_id, i, store_tag)\n",
    "            i += 1\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "df = df.apply(create_graph_object, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Spacy First Schema"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    1 non-null      object\n",
      "dtypes: object(1)\n",
      "memory usage: 136.0+ bytes\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                text\n0  Small people eat the large apples",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Small people eat the large apples</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search for the words in that specific order, or search for both words in the document in any order\n",
    "query = \"\"\"\n",
    "WITH  \"small\" as firstWord, \"apples\" as secondWord\n",
    "MATCH (t0:TagOccurrence)-[:HAS_NEXT*..2]-(t1:TagOccurrence)\n",
    "WHERE (t0.lemma = firstWord or t0.text = firstWord) AND (t1.lemma = secondWord or t1.text = secondWord)\n",
    "MATCH p = (t1)-[:IS_DEPENDENT]->(t0)<-[*2..2]-(at:AnnotatedText)\n",
    "return p, at, t1, t0\n",
    "\"\"\"\n",
    "\n",
    "query = \"\"\"\n",
    "WITH  \"small\" as firstWord, \"apples\" as secondWord\n",
    "MATCH (t0:TagOccurrence)-[:HAS_NEXT*..]-(t1:TagOccurrence)\n",
    "WHERE (t0.lemma = firstWord or t0.text = firstWord) AND (t1.lemma = secondWord or t1.text = secondWord)\n",
    "MATCH (t1)-[:IS_DEPENDENT]->(t0)<--(s:Sentence)<--(at:AnnotatedText)\n",
    "return s.text as text\n",
    "\"\"\"\n",
    "\n",
    "# Words next to each other, MATCH (t0:TagOccurrence)-[:HAS_NEXT]->(t1:TagOccurrence) also works\n",
    "query = \"\"\"\n",
    "WITH  \"small\" as firstWord, \"apples\" as secondWord\n",
    "MATCH (t0:TagOccurrence)-[:HAS_NEXT*0..1]-(t1:TagOccurrence)\n",
    "WHERE (t0.lemma = firstWord or t0.text = firstWord) AND (t1.lemma = secondWord or t1.text = secondWord)\n",
    "MATCH (t1)-[:IS_DEPENDENT]->(t0)<--(s:Sentence)<--(at:AnnotatedText)\n",
    "return s.text as text\n",
    "\"\"\"\n",
    "\n",
    "# Start and ends with\n",
    "query = \"\"\"\n",
    "WITH  \"small\" as firstWord, \"apples\" as secondWord\n",
    "MATCH (s:Sentence)-[:HAS_TOKEN]->(t0:TagOccurrence)-[HAS_NEXT*]->(t1:TagOccurrence)\n",
    "WHERE (t0.lemma = firstWord or t0.text = firstWord) AND (t1.lemma = secondWord or t1.text = secondWord) AND NOT ()-[:HAS_NEXT]->(t0)\n",
    "return DISTINCT s.text as text\n",
    "\"\"\"\n",
    "res = graph.run(query).to_data_frame()\n",
    "print(res.info())\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T22:43:41.729249300Z",
     "start_time": "2023-05-17T22:43:41.390118Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
