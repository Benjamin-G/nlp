{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Suppose that you would like to implement a tool that supports message writing, suggesting the next word while you are typing. Moreover, suppose that you would like the tool to learn from you or from a specific set of documents. Such a tool could be useful not only for providing message-writing assistance, but also for supporting spell checking, extracting common phrases, summarizing, and so on."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from py2neo import Graph\n",
    "\n",
    "# Connect to database\n",
    "url = \"bolt://localhost:7687\"\n",
    "username = os.getenv(\"NEO4J_USER\")\n",
    "password = os.getenv(\"NEO4J_PASSWORD\")\n",
    "database = \"masc\"\n",
    "\n",
    "graph = Graph(url, auth=(username, password), name=database)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "add_constraint = \"CREATE CONSTRAINT FOR (w:Word) REQUIRE w.value IS UNIQUE;\"\n",
    "graph.run(add_constraint)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# doesn't work here use desktop\n",
    "load_data_1 = \"\"\"\n",
    "LOAD CSV FROM \"file:///masc_sentences.tsv\" AS line FIELDTERMINATOR '\\t'\n",
    "CALL {\n",
    "    WITH line\n",
    "    WITH line[6] as sentence\n",
    "    WITH split(sentence, \" \") as words\n",
    "    FOREACH ( idx IN range(0,size(words)-2) |\n",
    "    MERGE (w1:Word {value:apoc.text.clean(words[idx])})\n",
    "    MERGE (w2:Word {value:apoc.text.clean(words[idx+1])})\n",
    "    MERGE (w1)-[r:NEXT]->(w2)\n",
    "      ON CREATE SET r.weight = 1\n",
    "      ON MATCH SET r.weight = r.weight + 1)\n",
    "} IN TRANSACTIONS OF 500 ROWS\n",
    "\"\"\"\n",
    "# without cleaning\n",
    "# Added 170037 labels, created 170037 nodes, set 2553105 properties, created 929137 relationships, completed after 261495 ms.\n",
    "# with cleaning\n",
    "# Added 97209 labels, created 97209 nodes, set 2480277 properties, created 806397 relationships, completed after 229257 ms.\n",
    "\n",
    "\n",
    "# New importing query that uses the sentence identifier\n",
    "# The word nodes are unique, so if you have millions of sentences, this schema will create supernodesâ€”that is,\n",
    "# nodes with millions of relationships coming in, going out, or both.\n",
    "load_data = \"\"\"\n",
    "LOAD CSV FROM \"file:///masc_sentences.tsv\" AS line FIELDTERMINATOR '\\t'\n",
    "CALL {\n",
    "    WITH line\n",
    "    WITH line[6] as sentence, line[2] as sentenceId\n",
    "    WITH split(sentence,\" \") as words, sentenceId\n",
    "    FOREACH ( idx IN range(0,size(words)-2) |\n",
    "    MERGE (w1:Word {value:apoc.text.clean(words[idx])})\n",
    "    MERGE (w2:Word {value:apoc.text.clean(words[idx+1])})\n",
    "    CREATE (w1)-[r:NEXT {sentence: sentenceId}]->(w2))\n",
    "} IN TRANSACTIONS OF 500 ROWS\n",
    "\"\"\"\n",
    "# Added 97209 labels, created 97209 nodes, set 2480277 properties, created 2383068 relationships, completed after 93746 ms.\n",
    "delete = \"\"\"\n",
    "CALL apoc.periodic.iterate(\n",
    "\"MATCH (p:Word) RETURN p\",\n",
    "\"DETACH DELETE p\", {batchSize:500})\n",
    "\"\"\"\n",
    "# Had to add masc_sentences.tsv to the DB server\n",
    "graph.run(load_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "MATCH (w:Word {value: \"how\"})-[e:NEXT]->(w2:Word)\n",
    "RETURN w2.value as next, e.weight as frequency\n",
    "ORDER BY frequency desc\n",
    "LIMIT 3\n",
    "\"\"\"\n",
    "\n",
    "res = graph.run(query).to_data_frame()\n",
    "res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "MATCH (w:Word)-[e:NEXT]->(w2:Word)\n",
    "RETURN apoc.text.clean(w2.value) as next, e.weight as frequency\n",
    "ORDER BY frequency desc\n",
    "LIMIT 100\n",
    "\"\"\"\n",
    "res = graph.run(query).to_data_frame()\n",
    "res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### sentence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "MATCH (w2:Word {value: \"know\"})-[r:NEXT]->(w3:Word {value: \"how\"})-[e:NEXT]-> (w4:Word)\n",
    "WHERE r.sentence = e.sentence\n",
    "RETURN w4.value as next, count(DISTINCT r) as frequency\n",
    "ORDER BY frequency desc\n",
    "LIMIT 3\n",
    "\"\"\"\n",
    "query = \"\"\"\n",
    "MATCH (w1:Word {value: \"you\"})-[a:NEXT]->(w2:Word {value: \"know\"})-[r:NEXT]->(w3:Word {value: \"how\"})-[e:NEXT]->(w4:Word)\n",
    "WHERE a.sentence = r.sentence AND r.sentence = e.sentence\n",
    "RETURN w4.value as next, count(DISTINCT r) as frequency\n",
    "ORDER BY frequency desc\n",
    "LIMIT 3\n",
    "\"\"\"\n",
    "res = graph.run(query).to_data_frame()\n",
    "res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Simple Spacy Example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    i = 1\n",
    "    for sentence in doc.sents:\n",
    "        print(\"-------- Sentence \", i, \"-----------\")\n",
    "        i += 1\n",
    "        for token in sentence:\n",
    "            print(token.idx, \"-\", token.text, \"-\", token.lemma_, \"-\", token.tag_)\n",
    "\n",
    "\n",
    "sentence = \"Marie Curie received the Nobel Prize in Physic in 1903. She became the first woman to win the prize.\"\n",
    "tokenize(sentence)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "\n",
    "# stanza.download(\"en\")\n",
    "\n",
    "def tokenize(text):\n",
    "    nlp = stanza.Pipeline()\n",
    "    # models_dir='stanfordnlp_resources')  # This sets up a default neural pipeline in English\n",
    "    doc = nlp(text)\n",
    "    i = 1\n",
    "    for sentence in doc.sentences:\n",
    "        print(\"--------Sentence \", i, \"-----------\")\n",
    "        i += 1\n",
    "        for token in sentence.tokens:\n",
    "            # print(token.pretty_print())\n",
    "            print(token.id, \"-\", token.text, \"-\", token.words[0].lemma)\n",
    "\n",
    "\n",
    "tokenize(\"Barack Obama was born in Hawaii.  He was elected president in 2008.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "import torch\n",
    "\n",
    "import gc\n",
    "\n",
    "\n",
    "def report_gpu():\n",
    "    print(torch.cuda.list_gpu_processes())\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "report_gpu()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataframe Ingestion Version"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query:\n",
      " MERGE (ann:AnnotatedText {id: $id, nlp_json: $nlp_json})\n",
      "        RETURN id(ann) as result\n",
      "    \n",
      "with params:  {'id': 1, 'nlp_json': \"{'text': 'Marie Curie received the Nobel Prize in Physic in 1903. She became the first woman to win the prize.', 'sents': [{'start': 0, 'end': 55}, {'start': 56, 'end': 100}], 'tokens': [{'id': 0, 'start': 0, 'end': 5, 'tag': 'NNP', 'pos': 'PROPN', 'morph': 'Number=Sing', 'lemma': 'Marie', 'dep': 'compound', 'head': 1}, {'id': 1, 'start': 6, 'end': 11, 'tag': 'NNP', 'pos': 'PROPN', 'morph': 'Number=Sing', 'lemma': 'Curie', 'dep': 'nsubj', 'head': 2}, {'id': 2, 'start': 12, 'end': 20, 'tag': 'VBD', 'pos': 'VERB', 'morph': 'Tense=Past|VerbForm=Fin', 'lemma': 'receive', 'dep': 'ROOT', 'head': 2}, {'id': 3, 'start': 21, 'end': 24, 'tag': 'DT', 'pos': 'DET', 'morph': 'Definite=Def|PronType=Art', 'lemma': 'the', 'dep': 'det', 'head': 5}, {'id': 4, 'start': 25, 'end': 30, 'tag': 'NNP', 'pos': 'PROPN', 'morph': 'Number=Sing', 'lemma': 'Nobel', 'dep': 'compound', 'head': 5}, {'id': 5, 'start': 31, 'end': 36, 'tag': 'NNP', 'pos': 'PROPN', 'morph': 'Number=Sing', 'lemma': 'Prize', 'dep': 'dobj', 'head': 2}, {'id': 6, 'start': 37, 'end': 39, 'tag': 'IN', 'pos': 'ADP', 'morph': '', 'lemma': 'in', 'dep': 'prep', 'head': 5}, {'id': 7, 'start': 40, 'end': 46, 'tag': 'NNP', 'pos': 'PROPN', 'morph': 'Number=Sing', 'lemma': 'Physic', 'dep': 'pobj', 'head': 6}, {'id': 8, 'start': 47, 'end': 49, 'tag': 'IN', 'pos': 'ADP', 'morph': '', 'lemma': 'in', 'dep': 'prep', 'head': 2}, {'id': 9, 'start': 50, 'end': 54, 'tag': 'CD', 'pos': 'NUM', 'morph': 'NumType=Card', 'lemma': '1903', 'dep': 'pobj', 'head': 8}, {'id': 10, 'start': 54, 'end': 55, 'tag': '.', 'pos': 'PUNCT', 'morph': 'PunctType=Peri', 'lemma': '.', 'dep': 'punct', 'head': 2}, {'id': 11, 'start': 56, 'end': 59, 'tag': 'PRP', 'pos': 'PRON', 'morph': 'Case=Nom|Gender=Fem|Number=Sing|Person=3|PronType=Prs', 'lemma': 'she', 'dep': 'nsubj', 'head': 12}, {'id': 12, 'start': 60, 'end': 66, 'tag': 'VBD', 'pos': 'VERB', 'morph': 'Tense=Past|VerbForm=Fin', 'lemma': 'become', 'dep': 'ROOT', 'head': 12}, {'id': 13, 'start': 67, 'end': 70, 'tag': 'DT', 'pos': 'DET', 'morph': 'Definite=Def|PronType=Art', 'lemma': 'the', 'dep': 'det', 'head': 15}, {'id': 14, 'start': 71, 'end': 76, 'tag': 'JJ', 'pos': 'ADJ', 'morph': 'Degree=Pos', 'lemma': 'first', 'dep': 'amod', 'head': 15}, {'id': 15, 'start': 77, 'end': 82, 'tag': 'NN', 'pos': 'NOUN', 'morph': 'Number=Sing', 'lemma': 'woman', 'dep': 'attr', 'head': 12}, {'id': 16, 'start': 83, 'end': 85, 'tag': 'TO', 'pos': 'PART', 'morph': '', 'lemma': 'to', 'dep': 'aux', 'head': 17}, {'id': 17, 'start': 86, 'end': 89, 'tag': 'VB', 'pos': 'VERB', 'morph': 'VerbForm=Inf', 'lemma': 'win', 'dep': 'advcl', 'head': 12}, {'id': 18, 'start': 90, 'end': 93, 'tag': 'DT', 'pos': 'DET', 'morph': 'Definite=Def|PronType=Art', 'lemma': 'the', 'dep': 'det', 'head': 19}, {'id': 19, 'start': 94, 'end': 99, 'tag': 'NN', 'pos': 'NOUN', 'morph': 'Number=Sing', 'lemma': 'prize', 'dep': 'dobj', 'head': 17}, {'id': 20, 'start': 99, 'end': 100, 'tag': '.', 'pos': 'PUNCT', 'morph': 'PunctType=Peri', 'lemma': '.', 'dep': 'punct', 'head': 12}]}\"}\n",
      "-------- Sentence  1 -----------\n",
      "32\n",
      "-------- Sentence  2 -----------\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = os.getenv(\"NEO4J_USER\")\n",
    "password = os.getenv(\"NEO4J_PASSWORD\")\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "\n",
    "def get_session(database):\n",
    "    return driver.session(database=database)\n",
    "\n",
    "\n",
    "text = \"Marie Curie received the Nobel Prize in Physic in 1903. She became the first woman to win the prize.\"\n",
    "df = pd.DataFrame([text], columns=[\"raw_text\"])\n",
    "\n",
    "\n",
    "def execute_query(query, params):\n",
    "    results = []\n",
    "    with get_session(\"spacy\") as session:\n",
    "        for items in session.run(query, params):\n",
    "            item = items[\"result\"]\n",
    "            results.append(item)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def create_annotated_text(nlp_json, id=1):\n",
    "    query = \"\"\"MERGE (ann:AnnotatedText {id: $id, nlp_json: $nlp_json})\n",
    "        RETURN id(ann) as result\n",
    "    \"\"\"\n",
    "    params = {\"id\": id, \"nlp_json\": nlp_json}\n",
    "    results = execute_query(query, params)\n",
    "    return results[0]\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def create_graph_object(row):\n",
    "    raw_text = row[\"raw_text\"]\n",
    "    for doc in nlp.pipe([raw_text], disable=[\"ner\"]):\n",
    "        row[\"nlp_json\"] = str(doc.to_json())\n",
    "        annotated_text = create_annotated_text(row[\"nlp_json\"])\n",
    "        i = 1\n",
    "        for sentence in doc.sents:\n",
    "            print(\"-------- Sentence \", i, \"-----------\")\n",
    "            print(annotated_text)\n",
    "            print(sentence.text)\n",
    "            # self.store_sentence(sentence, annotated_text, text_id, i, store_tag)\n",
    "            i += 1\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "df = df.apply(create_graph_object, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T18:44:12.181105Z",
     "start_time": "2023-05-17T18:44:09.093169600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
